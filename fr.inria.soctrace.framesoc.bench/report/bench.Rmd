---
title: "Framesoc Bench"
author: "Generoso Pagano"
date: "01/08/2015"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
---

```{r, echo=FALSE}
## RUN THIS BEFORE ALL

library("plyr")                                                                                                                                              
library("ggplot2")   

## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    require(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}

# utility to get trace data
get_trace <- function(data, s) {
  df <- data.frame(data)
  df <- df[df$size==s,]
  return(df)
}

```

# Intro

## Summary
Framesoc infrastructure benchmark, concerning trace import times and event reading times.

## Vocabulary
* Parameter: something influencing the behavior of the system.
* Factor: parameter you decide to variate during experiment.
* Level: value given to a factor.
* Metric: what we want to measure

# Environment

Hard disk details, obtained with hdparm.

```
[generoso@generosohp ~]$ sudo hdparm -i /dev/sdb1

/dev/sdb1:

 Model=Hitachi HDS721010CLA630, FwRev=JP4OA41A, SerialNo=JP2940N03MSP5V
 Config={ HardSect NotMFM HdSw>15uSec Fixed DTR>10Mbs }
 RawCHS=16383/16/63, TrkSize=0, SectSize=0, ECCbytes=56
 BuffType=DualPortCache, BuffSize=29999kB, MaxMultSect=16, MultSect=16
 CurCHS=16383/16/63, CurSects=16514064, LBA=yes, LBAsects=1953525168
 IORDY=on/off, tPIO={min:120,w/IORDY:120}, tDMA={min:120,rec:120}
 PIO modes:  pio0 pio1 pio2 pio3 pio4 
 DMA modes:  mdma0 mdma1 mdma2 
 UDMA modes: udma0 udma1 udma2 udma3 udma4 udma5 *udma6 
 AdvancedPM=no WriteCache=enabled
 Drive conforms to: unknown:  ATA/ATAPI-2,3,4,5,6,7
```

System details, obtained with custom script.

```
[generoso@generosohp ~]$ ./read_configuration.sh 
# OS details
Linux generosohp.imag.fr 3.9.10-100.fc17.x86_64 #1 SMP Sun Jul 14 01:31:27 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
# HW details
Number of CPUs: 12
CPU information (all cpus are equal):
- model name : Intel(R) Xeon(R) CPU E5-1660 0 @ 3.30GHz
- cache size : 15360 KB
- hyperthreading : active
Scaling governor: performance
RAM: 16360588 kB
```

# Benchmark
    
## Trace Import

### Description

* Factors: 
    + Trace size: measured in # of events, directly translatable in the DB size (MB)
    + Indexing: type of indexing (see levels)
* Levels
    + Trace size:
        - small: 1 Mevent (66.3 MB without index, 81.2 MB with index)
        - medium: 10 Mevent (692.6 MB without index, 843.9 MB with index)
        - big: 100 Mevent (7.5 GB without index, 9.3 GB with index)
    + Indexing:
        - none
        - time
        - both: time and event id on event param table
* Metric:
    + Import time
* Other parameters: 
    + fake traces created with the tool Temictli (available on https://github.com/soctrace-inria/framesoc.various)
    + the Temictli configuration file used is provided in this archive (./conf/temictli_bench.conf).
    + all the traces have 2 event parameters for each event (this has been chosen as an average value over many trace formats)
* Experiments:
    + small traces: 30 repetitions
    + medium traces: 10 repetitions
    + big traces: 5 repetitions
    + NOTE: as we will see below this number of repetition is largely enough to ensure very small confidence intervals.

### Analysis

Import time given the number of events and the indexing property.

```{r, echo=FALSE}                                                                                                                                              
# read data
eindex <- read.csv("../results/temictli_eindex.log")
levels(eindex$index) <- c("both", "time", "none")

old <- read.csv("../results/temictli_bench.log")
# replaces levels in the order, performing the substitutions
# levels(old$index) is "false" "true" 
# so all true will be assigned with both and all false with none
# the assignment below are not necessary
#old$index[old$index=="false"] <- "none"
#old$index[old$index=="true"] <- "time"
levels(old$index) <- c("none", "time", "both")

# bind two data frames
all <- rbind(old, eindex)

# aggregate
re <- summarySE(all, measurevar="time", groupvars=c("size", "index"))

# plot                                                                                                                                                             
ggplot(re, aes(x=size, y=time, colour=index)) +                 
geom_line() +
geom_errorbar(aes(ymin=time-se, ymax=time+se), width=.1) +                                                                                               
geom_point() +                                                                                                                                                 
scale_y_continuous(name="time (ms)") +                                                                                                                          
scale_x_continuous(name="# of events")        
```

Text representation of the above plot.

```{r, echo=FALSE}
re
```

## Trace reading

### Description

Note: for factors and levels already seen above (e.g. trace size), we omit the description,
since it is the same.

* Factors: 
    + Trace size
    + Indexing
    + Param (Reading parameters): boolean
    + Reading interval: the number of events per interval (all events, 100k events, ...)
* Levels
    + Trace size:
        - small
        - medium
        - big
    + Indexing
        - none
        - time
        - both: time and event id in event param table
    + Param: 
        - true
        - false
    + Interval
        - all events, labelled as interval=0 (only small and medium traces)
        - 100k events, labelled as interval=100000 (chosen as best compromise between speed and UI interactivity for the GANTT)
        - 20k events, labelled as interval=20000 (chosen as best compromise when loading param with event id index, TABLE). 
* Metric:
    + Total read time
    + Interval read time
* Other parameters: 
    + fake traces created above
    + the program used for reading (fr.inria.soctrace.framesoc.bench) is available on github:
    (https://github.com/soctrace-inria/framesoc.various)
* Experiments:
    + small traces: 30+ repetitions
    + medium traces: 10+ repetitions
    + big traces: only partial execution, to have the time to read an interval. No tests without interval (all events) because of memory limits.
    + NOTE: as we will see below this number of repetition is largely enough to ensure very small confidence intervals.

### Analysis

```{r, echo=FALSE}
# prepare data summaries

# merge old and new data
interval20k <- read.csv("../results/reader_smallint.log")
eindex <- read.csv("../results/reader_eindex.log")
levels(eindex$index) <- c("both", "time", "none")
old <- read.csv("../results/reader_second.log")
levels(old$index) <- c("none", "time")

# bind all data frames
all <- rbind(old, eindex, interval20k)

# total time summary
dftime <- summarySE(all, measurevar="total_time", groupvars=c("size", "index", "param", "interval"))

# interval summary
dfinterval <- summarySE(all, measurevar="interval_time", groupvars=c("size", "index", "param", "interval"))

# Factors: par, index
# read all, par, index
readall <- dftime[dftime$interval=='0',]
# read interval, par, index
readint <- dftime[dftime$interval=='100000',]

# Factors: interval, index
# read nopar, interval, index
readnopar <- dftime[dftime$param=='false',]
intervalnopar <- dfinterval[dfinterval$param=='false',]
# read par, interval, index
readpar <- dftime[dftime$param=='true',]
intervalreadpar <- dfinterval[dfinterval$param=='true',]

small_readall <- get_trace(readall, 1000000)
medium_readall <- get_trace(readall, 10000000)
big_readall <- get_trace(readall, 100000000)

small_readint <- get_trace(readint, 1000000)
medium_readint <- get_trace(readint, 10000000)
big_readint <- get_trace(readint, 100000000)

small_readnopar <- get_trace(readnopar, 1000000)
medium_readnopar <- get_trace(readnopar, 10000000)
big_readnopar <- get_trace(readnopar, 100000000)

small_readpar <- get_trace(readpar, 1000000)
medium_readpar <- get_trace(readpar, 10000000)
big_readpar <- get_trace(readpar, 100000000)

small_intervalnopar <- get_trace(intervalnopar, 1000000)
medium_intervalnopar <- get_trace(intervalnopar, 10000000)
big_intervalnopar <- get_trace(intervalnopar, 100000000)

small_intervalpar <- get_trace(intervalreadpar, 1000000)
medium_intervalpar <- get_trace(intervalreadpar, 10000000)
big_intervalpar <- get_trace(intervalreadpar, 100000000)

```

#### Summarized values

##### Total time
```{r, echo=FALSE}
dftime[c(1,2,3,4,6)]
```

##### Interval time
```{r, echo=FALSE}
dfinterval[c(1,2,3,4,6)]
```

#### Total read time (no interval reading), with and without param, with and without time index.

In this analysis we see that:

- reading parameters is more than 6x slower
- when reading all the events, the time index does not change anything (as intuition would say)
- reading all the events does not scale: for big traces there's not enough memory to perform the query (even with 10 GB reserved for Java Heap).

##### Small trace

```{r, echo=FALSE}
ggplot(data=small_readall, aes(x=index, y=total_time, fill=param)) + geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) +
scale_y_continuous(name="Total time (ms)")  
```

##### Medium trace

```{r, echo=FALSE}
ggplot(data=medium_readall, aes(x=index, y=total_time, fill=param)) + geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) +
scale_y_continuous(name="Total time (ms)")  
```

##### Big trace

Without interval (reading all), giving 10 GB of heap space to eclipse, it was impossible to run these tests

Got the following `OutOfMemoryError` error: 
      ```
      !ENTRY org.eclipse.core.jobs 4 2 2015-01-14 15:41:18.088
      !MESSAGE An internal error occurred during: "Framesoc Reader".
      !STACK 0
      java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.util.ArrayList.<init>(ArrayList.java:132)
        at java.util.ArrayList.<init>(ArrayList.java:139)
        at fr.inria.soctrace.lib.model.Event.<init>(Event.java:122)
      	at fr.inria.soctrace.lib.model.State.<init>(State.java:24)
      	at fr.inria.soctrace.lib.model.Event.createCategorizedEvent(Event.java:108)
      	at fr.inria.soctrace.lib.query.EventQuery.rebuildEvent(EventQuery.java:395)
      	at fr.inria.soctrace.lib.query.EventQuery.rebuildEvents(EventQuery.java:357)
      	at fr.inria.soctrace.lib.query.EventQuery.getList(EventQuery.java:208)
      	at fr.inria.soctrace.framesoc.bench.reading.FramesocReader.readAll(FramesocReader.java:166)
      	at fr.inria.soctrace.framesoc.bench.reading.FramesocReader.doExperiment(FramesocReader.java:147)
      	at fr.inria.soctrace.framesoc.bench.reading.FramesocReader.main(FramesocReader.java:131)
      	at fr.inria.soctrace.framesoc.bench.reading.FramesocReaderTool$1.run(FramesocReaderTool.java:20)
      	at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)
      ```

#### Total read time (interval reading, interval=100000), with and without param, with different indexing.

In this analysis we see that:

- when reading intervals, reading parameters kills performance even more.
- the time index is effective only when we don't read parameters.
- with an interval of 100000, the EVENT_ID index on EVENT_PARAM table (meaningful only when reading parameters) is useless.

##### Small trace

```{r, echo=FALSE}
ggplot(data=small_readint, aes(x=index, y=total_time, fill=param)) + geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) +
scale_y_continuous(name="Total time (ms)")  
```

##### Medium trace

```{r, echo=FALSE}
ggplot(data=medium_readint, aes(x=index, y=total_time, fill=param)) + geom_bar(stat="identity", position=position_dodge()) +
geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) +
scale_y_continuous(name="Total time (ms)")  
```

##### Big trace

- with interval=100000
    - without time index
        - with param, reading an interval is about 172 seconds, 47 h for the trace
        - without param, 61 seconds for an interval, 16 hours for the trace

    - with time index
        - with param, reading an interval is about 139 seconds, for the whole trace it would take 38 hours (TABLE)
        - without param, reading an interval is 250 milliseconds, so 4 minutes for the whole trace (GANTT)
        - NOTE: using param is so long because there is no index on the EVENT_ID of EVENT_PARAM table and the interval 
          used (100000 events) is big. See conclusions.    
        
    - with both indexes
        - with param, reading an interval is 133 seconds, so still 37 h for the whole trace (TABLE). 
          The index on EVENT_ID is useless when using an interval too big (100000). See conclusions.
        - without param is as above with time index (GANTT case). 
        
    
#### Total read time and interval time, with param, with different indexing (TABLE)

In the titles above, first results means: the average time to start getting results (get an interval in the interval case).

In this analysis we see that:

- using a smaller interval (20k events) enables the benefits of having the both indexes: timestamp index and EVENT_ID index (EVENT_PARAM table).
- with small interval (20k) and both indexes, the overhead of reading intervals (instead of the whole trace) and parameters is basically removed (actually visible only on small traces). 
    + For medium traces, for example, with both index we are 14x faster than having the simple time index.
    + For big traces, we pass from 38 hours to 5 minutes. (~450x faster)

##### Small trace 

Total read time.

```{r, echo=FALSE}
ggplot(data=small_readpar, aes(x=index, y=total_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```
 
First results.

```{r, echo=FALSE}
ggplot(data=small_intervalpar, aes(x=index, y=interval_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=interval_time-ci, ymax=interval_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

##### Medium trace 

Total read time.

```{r, echo=FALSE}
ggplot(data=medium_readpar, aes(x=index, y=total_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

 
First results.

```{r, echo=FALSE}
ggplot(data=medium_intervalpar, aes(x=index, y=interval_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=interval_time-ci, ymax=interval_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

##### Big traces

- with param and with interval=100k reading an interval is about 139 seconds, for the whole trace it would take 38 hours.
- with param and with interval=20k, with both indexes reading an interval takes only 300 ms, so 5 minutes for the whole trace.
    + this proves that the index on event id (EVENT_PARAM table) is really useful reading smaller intervals, also for big traces.

#### Total read time and interval time, without param, with and without time index (GANTT).

In this analysis we see that:

- Using interval reading, adds some overhead to total time
    + this is visible only on small traces and without time index
    + for example, for medium traces, with index, this overhead is about 60%, but, on the other side, we start seeing meaningful information (first interval) after only ~200 ms (instead of waiting the whole loading).
- Using a time index significantly reduces total read time, when reading intervals.
    + this is more and more visible as the trace size grows
    + for example, for big traces we pass from 16 hours to 4 minutes (~240x faster)


##### Small trace

```{r, echo=FALSE}
ggplot(data=small_readnopar, aes(x=index, y=total_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

Average time to start getting results (get an interval in the interval case)

```{r, echo=FALSE}
ggplot(data=small_intervalnopar, aes(x=index, y=interval_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=interval_time-ci, ymax=interval_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

##### Medium trace

```{r, echo=FALSE}
ggplot(data=medium_readnopar, aes(x=index, y=total_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=total_time-ci, ymax=total_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

Average time to start getting results (get an interval in the interval case)

```{r, echo=FALSE}
ggplot(data=medium_intervalnopar, aes(x=index, y=interval_time, fill=factor(interval)))  + 
         geom_bar(stat="identity", position=position_dodge()) + 
         geom_errorbar(aes(ymin=interval_time-ci, ymax=interval_time+ci), width=.1, position=position_dodge(.9)) + 
         scale_y_continuous(name="Total time (ms)")  
```

##### Big trace

- read all (interval==0): no data because of memory limits.
- with interval=100k
    - with index
        - without param, reading an interval is ~240 milliseconds, so about 4 minutes for the whole trace (GANTT)
    - without index
        - without param, 61 seconds for an interval, 16 hours for the trace

#### Conclusion

- Having parameters (without an index on EVENT_ID in the EVENT_PARAM table) kills performance in reading.
  This is because, even if we have a temporal index to load events, we still have to search *the whole* EVENT_PARAM
  table to link the params to the events. For this reason an index on the event id field of this table could solve
  the issue. However, the only use-case where parameters are loaded is the TABLE and this kind of visualization is
  NOT intended to be used for the WHOLE trace, but only on small subsets (after analyzing with the Gantt for example).
  For this reason we don't have by default this index yet. 

- Actually, even with an index on event id, in the EVENT_PARAM table, the performance with parameters does not change 
  if we keep interval=100k. On the contrary, using a smaller interval (20k) this index become really useful.
  This is because the query to load the parameters has the form: SELECT * from EVENT_PARAM where EVENT_ID IN (<list of 
  event ids>). If this list is too long, even using the index, we do not have gain. If this list is smaller (20k), we 
  have huge gains. Note that, if a small interval (20k) helps a lot when we have both indexes, it kills performance if 
  we don't have the event id index on EVENT_PARAM table.
  This behavior may suggest to adapt the interval size, when loading the TABLE, according to the presence of the index
  on event id in EVENT_PARAM table.
  
- Having an index on the timestamp makes sense only for interval reading. If we read all, it has no effect on reading.
  As the trace size increases, it becomes absolutely necessary. For example: reading the big trace using intervals and
  not loading parameters (GANTT) would take about 16 hours without index and only 4 minutes with the index. 

- Interval (100k) reading ([with index, without parameters] == GANTT) increments total time by a factor from 3 to 1.5 
  (decreasing as the trace size increase), but we can have a first meaningful feedback in about 250 ms (first 
  interval). Note that for big traces interval reading is compulsory, since we are limited by the memory.
